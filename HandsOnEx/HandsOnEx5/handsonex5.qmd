---
title: "Hands On Ex 5 - Visualising and Anlysing Text with R"

author: "Cheng Chun Chieh"

date: "7 May 2024"
date-modified: "last-modified"

format: html
execute: 
  echo: true
  eval: true
  warning: false
  freeze: true
  
editor: visual
---

# 1. Overview and Getting Started

In this hands-on exercise, we will learn how to visualise and analyse text data using R.

By the end of this hands-on exercise, we will be able to:

-   understand tidytext framework for processing, analysing and visualising text data,

-   write function for importing multiple files into R,

-   combine multiple files into a single data frame,

-   clean and wrangle text data by using tidyverse approach,

-   visualise words with Word Cloud,

-   compute term frequency–inverse document frequency (TF-IDF) using tidytext method, and

-   visualising texts and terms relationship.

## 1.1 Installing the Packages

In this hands-on exercise, the following R packages for handling, processing, wrangling, analysing and visualising text data will be used:

-   tidytext, tidyverse (mainly readr, purrr, stringr, ggplot2)

-   widyr,

-   wordcloud and ggwordcloud,

-   textplot (required igraph, tidygraph and ggraph, )

-   DT,

-   lubridate and hms.

```{r}
pacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,tidyverse, tidygraph, ggraph, igraph)
```

## 1.2 Importing Multiple Text Files

The files are stored in a folder named 20news under data.

```{r}
news20 <- "data/20news/"
```

Defining a function to read all files into a dataframe:

```{r}

read_folder <- function(infolder) {
  tibble(file = dir(infolder, 
                    full.names = TRUE)) %>%
    mutate(text = map(file, read_lines)) %>%
    transmute(id = basename(file), text) %>%
    unnest(text)
}
```

Reading all the messages in the folder:

```{r}
raw_text <- tibble(folder = 
                     dir(news20, 
                         full.names = TRUE)) %>%
  mutate(folder_out = map(folder, 
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder), id, text)


write_rds(raw_text, "data/rds/news20.rds")

```

::: callout-note
[`read_lines()`](https://readr.tidyverse.org/reference/read_lines.html) of [**readr**](https://readr.tidyverse.org/) package is used to read up to n_max lines from a file.

-   [`map()`](https://purrr.tidyverse.org/reference/map.html) of [**purrr**](https://purrr.tidyverse.org/) package is used to transform their input by applying a function to each element of a list and returning an object of the same length as the input.

-   [`unnest()`](https://tidyr.tidyverse.org/reference/nest.html) of **dplyr** package is used to flatten a list-column of data frames back out into regular columns.

-   [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html) of **dplyr** is used to add new variables and preserves existing ones;

-   [`transmute()`](https://dplyr.tidyverse.org/reference/mutate.html) of **dplyr** is used to add new variables and drops existing ones.

-   [`read_rds()`](https://readr.tidyverse.org/reference/read_rds.html) is used to save the extracted and combined data frame as rds file for future use.
:::

## 1.3 Initial EDA

```{r}
raw_text %>%
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y = NULL)
```

# 2. Using tidytext

-   Using tidy data principles in processing, analysing and visualising text data.

-   Much of the infrastructure needed for text mining with tidy data frames already exists in packages like ‘dplyr’, ‘broom’, ‘tidyr’, and ‘ggplot2’.

Figure below shows the workflow using tidytext approach for processing and visualising text data.

![](images/clipboard-71002374.png)

## 2.1 **Removing header and automated email signatures**

Notice that each message has some structure and extra text that we don’t want to include in our analysis. For example, every message has a header, containing field such as “from:” or “in_reply_to:” that describe the message. Some also have automated email signatures, which occur after a line like “–”.

```{r}
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(
           text, "^--")) == 0) %>%
  ungroup()
```

::: callout-note
-   [`cumsum()`](https://rdrr.io/r/base/cumsum.html) of base R is used to return a vector whose elements are the cumulative sums of the elements of the argument.

-   [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** is used to detect the presence or absence of a pattern in a string.
:::

## 2.2 **Removing lines with nested text representing quotes from other users**

Regular expressions are used to remove with nested text representing quotes from other users.

See the link for more details: <https://www.datacamp.com/tutorial/regex-r-regular-expressions-guide>

```{r}
cleaned_text <- cleaned_text %>%
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
         | text == "",
         !str_detect(text, 
                     "writes(:|\\.\\.\\.)$"),
         !str_detect(text, 
                     "^In article <")
  )
```

::: callout-note
-   [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** is used to detect the presence or absence of a pattern in a string.

-   [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) of **dplyr** package is used to subset a data frame, retaining all rows that satisfy the specified conditions.
:::

## 2.3 Text Data Processing

In this code chunk below, [`unnest_tokens()`](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens, while [`stop_words()`](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words.

```{r}
usenet_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)
```

Now that we’ve removed the headers, signatures, and formatting, we can start exploring common words. For starters, we could find the most common words in the entire dataset, or within particular newsgroups.

```{r}
usenet_words %>%
  count(word, sort = TRUE)
```

Instead of counting individual word, you can also count words within by newsgroup by using the code chunk below.

```{r}
words_by_newsgroup <- usenet_words %>%
  count(newsgroup, word, sort = TRUE) %>%
  ungroup()
```

## 2.4 Visualising words in newsgroups

In this code chunk below, `wordcloud()` of **wordcloud** package is used to plot a static wordcloud.

```{r}
wordcloud(words_by_newsgroup$word,
          words_by_newsgroup$n,
          max.words = 300)
```

## 2.5 Visualising Words using ggwordcloud package

```{r}
set.seed(1234)

words_by_newsgroup %>%
  filter(n > 0) %>%
ggplot(aes(label = word,
           size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap(~newsgroup)
```

# 3. Concept of TF-IDF

-   [tf–idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection of [corpus](https://en.wikipedia.org/wiki/Text_corpus).

    ![](images/clipboard-4176247080.png){width="315" height="61"}

## 3.1 Computing tf-idf

The code chunk below uses *bind_tf_idf()* of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.

```{r}
tf_idf <- words_by_newsgroup %>%
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf_idf))
```

## 3.2 Visualising tf-idf as interactive data table

The code chunk below uses `datatable()` of DT package to create a html table that allows pagination of rows and columns.

```{r}
DT::datatable(tf_idf, filter = 'top') %>% 
  formatRound(columns = c('tf', 'idf', 
                          'tf_idf'), 
              digits = 3) %>%
  formatStyle(0, 
              target = 'row', 
              lineHeight='25%')
```

::: callout-note
-   `filter()` argument is used to turn control the filter UI.

-   `formatRound()` is used to customise the values format. The argument *digits* define the number of decimal places.

-   `formatStyle()` is used to customise the output table. In this example, the arguments *target* and *lineHeight* are used to reduce the line height by 25%.
:::

To learn more about customising DT’s table, visit this [link](https://rstudio.github.io/DT/functions.html).

## 3.3 Visualising with bar charts

Facet bar charts technique is used to visualise the tf-idf values of science related newsgroup.

```{r}
tf_idf %>%
  filter(str_detect(newsgroup, "^sci\\.")) %>%
  group_by(newsgroup) %>%
  slice_max(tf_idf, 
            n = 12) %>%
  ungroup() %>%
  mutate(word = reorder(word, 
                        tf_idf)) %>%
  ggplot(aes(tf_idf, 
             word, 
             fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, 
             scales = "free") +
  labs(x = "tf-idf", 
       y = NULL) +
  theme_minimal()
```

## 3.4 **Counting and correlating pairs of words with the widyr package**

-   To count the number of times that two words appear within the same document, or to see how correlated they are.

-   Most operations for finding pairwise counts or correlations need to turn the data into a wide matrix first.

-   [**widyr**](https://cran.r-project.org/web/packages/widyr/) package first ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result.

![](images/clipboard-313891623.png){width="364"}

In this code chunk below, `pairwise_cor()` of **widyr** package is used to compute the correlation between newsgroup based on the common words found.

```{r}
newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, 
               word, 
               n, 
               sort = TRUE)
```

## 3.5 Visualising correlation as network

```{r}
set.seed(2017)

newsgroup_cors %>%
  filter(correlation > .025) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, 
                     width = correlation)) +
  geom_node_point(size = 6, 
                  color = "lightblue") +
  geom_node_text(aes(label = name),
                 color = "red",
                 repel = TRUE) +
  theme_void()
```

## 3.6 Bigram

Bigram are two words coming together in the corpus(the entire collection of words/sentences).

```{r}
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, 
                text, 
                token = "ngrams", 
                n = 2)
```

```{r}
bigrams
```

Counting bigrams:

```{r}
bigrams_count <- bigrams %>%
  filter(bigram != 'NA') %>%
  count(bigram, sort = TRUE)

bigrams_count
```

Cleaning bigram:

The code chunk below is used to separate the bigram into two words:

```{r}
bigrams_separated <- bigrams %>%
  filter(bigram != 'NA') %>%
  separate(bigram, c("word1", "word2"), 
           sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_filtered
```

Count the bigram again:

```{r}
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
```

## 3.7 Creating a network graph from bigram data frame

In the code chunk below, a network graph is created by using `graph_from_data_frame()` of **igraph** package.

```{r}
bigram_graph <- bigram_counts %>%
  filter(n > 3) %>%
  graph_from_data_frame()
bigram_graph
```

## 3.8 Plotting the bigram

**ggraph** package is used to plot the bigram:

```{r}
set.seed(1234)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), 
                 vjust = 1, 
                 hjust = 1)
```

Revised Version:

```{r}
set.seed(1234)

a <- grid::arrow(type = "closed", 
                 length = unit(.15,
                               "inches"))

ggraph(bigram_graph, 
       layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), 
                 show.legend = FALSE,
                 arrow = a, 
                 end_cap = circle(.07,
                                  'inches')) +
  geom_node_point(color = "lightblue", 
                  size = 5) +
  geom_node_text(aes(label = name), 
                 vjust = 1, 
                 hjust = 1) +
  theme_void()
```

# 4. References

### **Core Readings**

-   [A beginner’s guide to using text in data visualization](https://flourish.studio/blog/text-in-data-visualization/)

-   [The pros and cons of word clouds as visualizations](https://www.visioncritical.com/pros-and-cons-word-clouds-visualizations/)

#### widyr

-   Reference guide

    -   [widyr: Widen, process, and re-tidy a dataset](https://cran.r-project.org/web/packages/widyr/vignettes/intro.html)

    -   [United Nations Voting Correlations](https://cran.r-project.org/web/packages/widyr/vignettes/united_nations.html)
