{
  "hash": "8d1cce45f037ca09ca7aa2ccc18b46ab",
  "result": {
    "markdown": "---\ntitle: \"In Class Ex 5 - Text Analytics\"\n\nauthor: \"Cheng Chun Chieh\"\n\ndate: \"11 May 2024\"\ndate-modified: \"last-modified\"\n\nformat: html\nexecute: \n  echo: true\n  eval: true\n  warning: false\n  freeze: true\n  \neditor: visual\n---\n\n\n# 1. Overview and Getting Started\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidytext, tidyverse, readtext, quanteda, ggwordcloud)\n```\n:::\n\n\nCan refer to the following links for info about the packages:\n\n-   quanteda - <https://quanteda.io/articles/quickstart.html>\n\n-   readtext - <https://readtext.quanteda.io/articles/readtext_vignette.html>\n\n## 1.1 Importing Text Data using readtext\n\n\n::: {.cell}\n\n```{.r .cell-code}\narticles <- \"data/articles/*\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data <- readtext(articles)\n```\n:::\n\n\n## 1.2 Corpus \n\n### **1.2.1 Working with a quanteda corpus**\n\n#### **Corpus principles**\n\nA corpus is designed to be a “library” of original documents that have been converted to plain, UTF-8 encoded text, and stored along with meta-data at the corpus level and at the document-level. We have a special name for document-level meta-data: *docvars*. These are variables or features that describe attributes of each document.\n\nA corpus is designed to be a more or less static container of texts with respect to processing and analysis. This means that the texts in corpus are not designed to be changed internally through (for example) cleaning or pre-processing steps, such as stemming or removing punctuation. Rather, texts can be extracted from the corpus as part of processing, and assigned to new objects, but the idea is that the corpus will remain as an original reference copy so that other analyses – for instance those in which stems and punctuation were required, such as analysing a reading ease index – can be performed on the same corpus.\n\nA corpus is a special form of character vector, meaning most functions that work with a character input will also work on a corpus. But a corpus object (as do other **quanteda** core objects) has its own convenient print method.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_text <- corpus(text_data)\nsummary(corpus_text,5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 338 documents, showing 5 documents:\n\n                                   Text Types Tokens Sentences\n Alvarez PLC__0__0__Haacklee Herald.txt   206    433        18\n    Alvarez PLC__0__0__Lomark Daily.txt   102    170        12\n   Alvarez PLC__0__0__The News Buoy.txt    90    200         9\n Alvarez PLC__0__1__Haacklee Herald.txt    96    187         8\n    Alvarez PLC__0__1__Lomark Daily.txt   241    504        21\n```\n:::\n:::\n\n\n## 1.3 Cleaning Text \n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words <- text_data %>%\n  unnest_tokens(word, text) %>%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n```\n:::\n\n\nDoing a word count:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nreadtext object consisting of 3260 documents and 0 docvars.\n# A data frame: 3,260 × 3\n  word             n text     \n  <chr>        <int> <chr>    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,254 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_by_doc_id <- usenet_words %>%\n  count(doc_id, word, sort = TRUE) %>%\n  ungroup()\n```\n:::\n\n\n## 1.4 Splitting up the doc_id \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data_split <- text_data %>%\n  mutate(Company = str_extract(doc_id, \"^[^_]+\"),\n         News_Agencies = str_extract(doc_id, \"(?<=__)[^_]+(?=\\\\.txt)\"))\n```\n:::\n\n\n-   **`(?<=__)`** is a positive lookbehind assertion that ensures the match occurs after \"\\_\\_\".\n\n-   **`[^_]+`** matches one or more characters that are not underscores, representing the news agency.\n\n-   **`(?=\\\\.txt)`** is a positive lookahead assertion that ensures the match occurs before \".txt\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data_splitted <- text_data %>%\n  separate_wider_delim(\"doc_id\",\n                       delim=\"__0__\",\n                       names = c(\"X\",\"Y\"),\n                       too_few = \"align_end\"\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words1 <- text_data_split %>%\n  unnest_tokens(word, text) %>%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n\nwords_by_news_agencies <- usenet_words1 %>%\n  count(News_Agencies, word, sort = TRUE) %>%\n  ungroup()\n```\n:::\n\n\n# 2. Importing json files \n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, graphlayouts, ggforce, skimr, tidytext, tidyverse, tidyr)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmc1_data <- fromJSON (\"data/mc1.json\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmc2_data <- fromJSON (\"data/mc2.json\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmc1_data_node <- mc1_data[[\"nodes\"]]\n\nmc1_data_links <- mc1_data[[\"links\"]]\n```\n:::\n\n\n## 2.2 Importing mc3 data \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify the file paths\ninput_file <- \"data/mc3.json\"\noutput_file <- \"data/mc3_processed.json\"\n\n# Read the JSON file\njson_data <- readLines(input_file)\n\n# Replace NaN with null\njson_data <- gsub(\"NaN\", \"null\", json_data)\n\n# Write the updated JSON data back to a file\nwriteLines(json_data, con = output_file)\n\n# Now you can read the processed JSON file into R\nmc3_data <- fromJSON(output_file)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}